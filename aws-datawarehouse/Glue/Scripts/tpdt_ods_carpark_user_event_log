import sys
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from datetime import datetime, date, time, timezone,timedelta
from pyspark.sql.functions import *
from pyspark.sql.types import StringType
from pyspark.sql.types import IntegerType
import pandas as pd
import numpy as np
import json
from datetime import datetime, date, time, timezone,timedelta


args = getResolvedOptions(sys.argv, ['JOB_NAME', 'env'])
##Initialization
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)


##Input and Output Config
env = args['env']
table_name = args['JOB_NAME']
if env == 'prod':
    db01="tpdt_01replica"
    db03="tpdt_03dw"
    db02="tpdt_02replica"
    output_path="s3://tpdt-parking/"+table_name+"/"
    staging_path="s3://tpdt-staging/"
    dimension_path="s3://tpdt-dimension/"
else:
    db01="tpdt_01replica_"+env
    db03="tpdt_03dw_"+env
    output_path="s3://tpdt-parking-"+env+"/"+table_name+"/"
    staging_path="s3://tpdt-staging-"+env+"/"
    dimension_path="s3://tpdt-dimension-"+env+"/"

prefix="shkpmalls_carpark_"



## Source Extraction
##source table : member
t_member = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = 'shkpmalls_vip_member', transformation_ctx = "member")
member_df = t_member.toDF().select(col('member_id').alias('memberId')).filter("status = 'V'")
member_df.createOrReplaceTempView("member")
print("Source Extraction Finished: Member...")


##source table : device_info
t_event_log = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'carpark_user_event_log', transformation_ctx = "carpark_user_event_log")
event_log_df = t_event_log.toDF().select('event_time',
                                       'log_id',
                                       'type',
                                       'carpark_id',
                                       'card_type',
                                       'member_id',
                                       trim(col('lpn')).alias('lpn')
                                       )
print("Source Extraxtion Finished: carpark_user_event_log...")


event_log_df_final = event_log_df.join(member_df, upper(event_log_df.member_id) == upper(member_df.memberId), how='left') \
    .withColumn('lpn_final', when(col('memberId').isNotNull() & col('lpn').isNotNull(), sha2(upper(col('lpn')), 256)).otherwise(col('lpn'))) \
    .select('event_time',
            'log_id',
            'type',
            'carpark_id',
            'card_type',
            'member_id',
            col('lpn_final').alias('lpn'))
event_log_df_final.createOrReplaceTempView("event_log_df_final")

spark.conf.set("spark.sql.session.timeZone", "UTC")

## Loading Result to S3
print("Saving Result into target destination...")

event_log_df_final.write.format('parquet').mode('overwrite').option("header",True).save(output_path)
print(f"Result Saved in {output_path}...")

job.commit()