import sys
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from datetime import datetime, date, time, timezone,timedelta 
from pyspark.sql.functions import udf, col, when, lit
from pyspark.sql.types import StringType
from pyspark.sql.types import IntegerType 
import pandas as pd
import numpy as np
import json
from datetime import datetime, date, time, timezone,timedelta 


args = getResolvedOptions(sys.argv, ['JOB_NAME', 'env'])
##Initialization
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)


##Input and Output Config
env = args['env']
table_name = args['JOB_NAME']
if env == 'prod':
    db01="tpdt_01replica"
    db03="tpdt_03dw"
    db02="tpdt_02replica"
    output_path="s3://tpdt-stampmission/"
    staging_path="s3://tpdt-staging/"
    dimension_path="s3://tpdt-dimension/"
else:
    db01="tpdt_01replica_"+env
    db03="tpdt_03dw_"+env
    output_path="s3://tpdt-stampmission-"+env+"/"
    staging_path="s3://tpdt-staging-"+env+"/"
    dimension_path="s3://tpdt-dimension-"+env+"/"


prefix="shkpmalls_stamp_"



## Source Extraction
##source table : member_mission_record
t_member_mission_record = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'member_mission_record', transformation_ctx = "member_mission_record")
member_mission_record_df = t_member_mission_record.toDF()
member_mission_record_df.createOrReplaceTempView("ods_mission_record")
print("Source Extraxtion Finished: member_mission_record...")

##source table : missions
t_missions = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'missions', transformation_ctx = "missions")
missions_df = t_missions.toDF()
missions_df.createOrReplaceTempView("ods_mission")
print("Source Extraxtion Finished: missions...")

##source table : mission_completed
t_mission_completed = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'mission_completed', transformation_ctx = "mission_completed")
mission_completed_df = t_mission_completed.toDF()
mission_completed_df.createOrReplaceTempView("mission_completed_df")
print("Source Extraxtion Finished: mission_completed...")

##source table : mission_redemption
t_mission_redemption = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'mission_redemption', transformation_ctx = "mission_redemption")
mission_redemption_df = t_mission_redemption.toDF()
print("Source Extraxtion Finished: mission_redemption...")

# Table for tableau
redeemed_member_df = spark.sql("""
Select Distinct missionid as mapping_missionid, vid as mapping_member_id, 
       max(Case when is_redeemed = 'yes' then 1 else 0 end) as is_redeemed
from mission_completed_df
group by missionid, vid
""")
redeemed_member_df.createOrReplaceTempView("tmp_mission_redemption")

tableau_df = spark.sql("""
Select a.mission_type, a.missionid, a.vid, 
			count(*) as no_record,
			case when b.mission_type='spending' then spending_total_no_receipt
			when b.mission_type='reward' then reward_total_redeem
            end as required_times,
			sum(redeemedBonusPoints) redeemedBonusPoints,
            case when c.mapping_member_id is not null then 'Y' else 'N' end as completed,
            is_redeemed
	from 		ods_mission_record a
	left join 	ods_mission	b on a.missionid=b.ID
	left join 	tmp_mission_redemption c on a.missionid=c.mapping_missionid and a.vid=c.mapping_member_id
	group by 	a.mission_type, a.missionid, a.vid, required_times, completed, is_redeemed

""")

spark.conf.set("spark.sql.session.timeZone", "UTC")


## Loading Result to S3
print("Saving Result into target destination...")

member_mission_record_df.write.format('parquet').partitionBy("missionid").mode('overwrite').option("header",True).save(output_path+'/tpdt_ods_member_mission_record')

missions_df.write.format('parquet').mode('overwrite').option("header",True).save(output_path+'/tpdt_ods_mission')

mission_completed_df.write.format('parquet').partitionBy("missionid").mode('overwrite').option("header",True).save(output_path+'/tpdt_ods_mission_completed')

mission_redemption_df.write.format('parquet').partitionBy("missionid").mode('overwrite').option("header",True).save(output_path+'/tpdt_ods_mission_redemption')

tableau_df.write.format('parquet').partitionBy("missionid").mode('overwrite').option("header",True).save(output_path+'/tpdt_tableau_mission')
print(f"Result Saved in {output_path}...")


job.commit()