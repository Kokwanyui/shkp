import sys
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from datetime import datetime, date, time, timezone,timedelta 
from pyspark.sql.functions import *
from pyspark.sql.types import StringType
from pyspark.sql.types import IntegerType 
import pandas as pd
import numpy as np
import json
from datetime import datetime, date, time, timezone,timedelta 
from pyspark.sql.window import Window


args = getResolvedOptions(sys.argv, ['JOB_NAME', 'env'])
##Initialization
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
 

##Input and Output Config
env = args['env']
table_name = args['JOB_NAME']

if env == 'prod':
    db01="tpdt_01replica"
    db02="tpdt_02replica"
    db03="tpdt_03dw"
    db03sub="tpdt_03sub"
    output_path="s3://tpdt-staging/"+table_name+'/'
    staging_path="s3://tpdt-staging/"
    dimension_path="s3://tpdt-dimension/"
    
else:
    db01="tpdt_01replica_"+env
    db03="tpdt_03dw_"+env
    db03sub="tpdt_03sub_"+env
    output_path="s3://tpdt-staginge-1-"+env+"/"+table_name+'/'
    staging_path="s3://tpdt-staging-"+env+"/"
    dimension_path="s3://tpdt-dimension-"+env+"/"
    

prefix="shkpmalls_vip_"
prefix03='tpdt_'



t_member_setting = glueContext.create_dynamic_frame.from_catalog(database = db01, table_name = prefix+'member_setting', transformation_ctx = "member_setting")
member_setting_df = t_member_setting.toDF().withColumn('snapshot_datetime', current_timestamp() + expr('INTERVAL 8 HOURS'))\
                                           .filter("to_date(updated_date) = date_sub(CAST(current_timestamp() as DATE), 1) or to_date(created_date) = date_sub(CAST(current_timestamp() as DATE), 1)")


## Loading Result to S3 -----------------------------------------------------------------------------------------------------------------------------------------------------------
print("Saving Result into target destination...")
member_setting_df.write.format('parquet').mode('append').option("header",True).save(output_path)
print(f"Result Saved in {output_path}...")


