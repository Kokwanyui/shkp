import pandas as pd
import boto3
import time
import datetime
import json


def run_athena(script, db, output_location):
    """Execute Athena Query and get execution_id"""
    client = boto3.client('athena')
    
    execution_id = client.start_query_execution(
        QueryString=script,
        QueryExecutionContext={
            'Database': db
        },
        ResultConfiguration={
            'OutputLocation': output_location,
        }
    )['QueryExecutionId']
    
    return execution_id


def get_filename(athena_execution_id, max_execution_time):
    """Check Running Status and get final result path"""
    client = boto3.client('athena')
    state = 'RUNNING'
    
    while (max_execution_time > 0 and state in ['RUNNING', 'QUEUED']):
        max_execution_time = max_execution_time - 1
        response = client.get_query_execution(QueryExecutionId=athena_execution_id)
        
        if 'QueryExecution' in response and 'Status' in response['QueryExecution'] and 'State' in response['QueryExecution']['Status']:
            state = response['QueryExecution']['Status']['State']
            if state == 'FAILED':
                return json.loads(json.dumps(response, default=str))
            elif state == 'SUCCEEDED':
                s3_path = response['QueryExecution']['ResultConfiguration']['OutputLocation']
                return s3_path
        time.sleep(20)


def clean_s3(s3_bucket, s3_folder):
    """clean all objects in folder"""
    s3 = boto3.resource('s3')
    
    clean_bucket = s3.Bucket(s3_bucket)
    for obj in clean_bucket.objects.filter(Prefix=s3_folder):
        s3.Object(s3_bucket,obj.key).delete()


def split_csv(bucket, csv_path, splited_csv_folder, abnormal_type):
    """split raw csv to excel"""
    
    df = pd.read_csv(csv_path)
    malls = df.property_id.unique().tolist()
    
    for mall in malls:
        mall_df = df[df['property_id'] == mall]
        
        if abnormal_type == 'CCC-per-receipt-ceiling-above-27000' or abnormal_type == 'API-per-receipt-ceiling-above-9000':
            mall_df = mall_df.sort_values(by='receipt_amount', ascending=False)
        else:
            pass
        
        if len(mall_df) > 0:
            today = now=datetime.datetime.now().date().strftime("%Y%m%d")
            folder_by_mall = f's3://{bucket}/{splited_csv_folder}malls'
            csv_output_path = f'{folder_by_mall}/{mall}_{abnormal_type}_{today}.csv'
                
            clean_s3(bucket, folder_by_mall)
            mall_df.to_csv(csv_output_path, index = False, encoding='utf-8')
        else:
            pass




# Main Function
bucket = 'tpdt-automation'
DATABASE = 'tpdt_03automation'
max_execution=5
target_views = ['abnormal_ccc_invoice', 'abnormal_ocr_invoice', 'abnormal_spend_member']

for target_view in target_views:
    if target_view == 'abnormal_ccc_invoice':
        abnormal_type = 'CCC-per-receipt-ceiling-above-27000'
    elif target_view == 'abnormal_ocr_invoice':
        abnormal_type = 'API-per-receipt-ceiling-above-9000'
    elif target_view == 'abnormal_spend_member':
        abnormal_type = 'abnormal_spending'
        
    # Config
    folder = f'abnormal_log/to_onedrive/{target_view}/raw/'
    output_path = f"s3://{bucket}/{folder}"
    splited_csv_folder = f'abnormal_log/to_onedrive/{target_view}/'
    query_string = f'Select * from {DATABASE}.{target_view}'
        
    # Clean S3 previous files
    clean_s3(bucket, splited_csv_folder)
    
    # Athena to S3
    execution_id = run_athena(query_string, DATABASE, output_path)
    final_path = get_filename(execution_id, max_execution)
    
    # Split CSV to excel
    split_csv(bucket, final_path, splited_csv_folder, abnormal_type)
    